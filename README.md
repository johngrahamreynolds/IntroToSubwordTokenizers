# IntroToSubwordTokenizers
This repo houses notebooks following a pedagogical approach to constructing the 3 most common subword tokenizers: BPE, WordPiece, and Unigram. These serve as a simple immediate reference for training subword tokenizers to contain a vocabulary that may be useful in eventually pre-training LLMs with a unique tokenization method.

**Note**: The emphasis here is on the pedagogical approach. That is, we construct these tokenization algorithms using *Python code only*. Should we wish to implement these methods for training subword tokenizers on a large corpus of text, best practice would ideally implement some of the underlying functionality in a language like Rust where optimized efficieny can be made manifest through advanced 'fast' parallelization. An intermediate workaround for this 'slow' Python implementation is to rather make use of a pre-defined tokenizer (GPT-2 for BPE, BERT for WordPiece, T5/XLNet for Unigram, etc.) and the ðŸ¤— Tokenizers `train_new_from_iterator()` function which indeed makes use of a RUST backend.
