# IntroToSubwordTokenizers
This repo houses notebooks following a pedagogical approach to constructing the 3 most common subword tokenizers: BPE, WordPiece, and Unigram. These notebooks serve as a simple immediate reference for training subword tokenizers to contain a vocabulary that may be useful in eventually pre-training LLMs with a unique tokenization method. Lastly, we add a file outlining the process for building a unique custom subword tokenizer brick by brick. These notebooks makes use of the ðŸ¤— Tokenizers library.

**Note**: The emphasis here is on the pedagogical approach. That is, we construct these tokenization algorithms using *Python code only*. Should we wish to implement these methods for training subword tokenizers on a large corpus of text, best practice would ideally implement some of the underlying functionality in a language like Rust where optimized efficieny can be made manifest through advanced 'fast' parallelization. 

An intermediate workaround for this 'slow' Python implementation is to rather make use of a pre-defined tokenizer (GPT-2 for BPE, BERT for WordPiece, T5/XLNet for Unigram, etc.) and the ðŸ¤— Tokenizers `train_new_from_iterator()` function which indeed makes use of a RUST backend. Perhaps even better, one can use the `train_from_iterator()` function on a custom built `Tokenizer` object to fully train a unique subword tokenizer; after such a training, one may wrap their custom tokenizer in a `PreTrainedTokenizerFast` class for complete compatibility with the rest of the ðŸ¤— ecosystem.
